{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stimuli and session template setup example notebook\n",
    "This notebook provides a minimal example on how to set up the stimuli and upload the sequence of trials to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from upload_to_s3 import upload_stim_to_s3, get_filepaths\n",
    "from experiment_config import experiment_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the names for the experiment and the iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"Physion_V1_5\"\n",
    "DATASET = \"Dominoes\"\n",
    "TASK = \"OCP\"\n",
    "ITERATION = \"1\"\n",
    "EXPERIMENT = DATASET + \"_\" + TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The experiment should be reachable under the following URL:\n",
      "https://cogtoolslab.org:8899/OCP/index.html?projName=Physion_V1_5&expName=Dominoes_OCP&iterName=1\n"
     ]
    }
   ],
   "source": [
    "PORT = 8899 # which port did we use when launching `app.js` (ie. the `--gameport` flag)\n",
    "print(\"The experiment should be reachable under the following URL:\")\n",
    "print(\"https://cogtoolslab.org:{}/{}/index.html?projName={}&expName={}&iterName={}\".format(PORT,TASK,PROJECT,EXPERIMENT,ITERATION))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide metadata and locations of the stimuli files\n",
    "for a simple data directory with all to-be-uploaded files in one directory,  data_path is in the form /path/to/your/data\n",
    "    \n",
    "For a multi-level directory structure, you will need to use glob ** notation in data_path to index all the relevant files. something like:\n",
    "- `/path/to/your/files/**/*` (this finds all the files in your directory structure)\n",
    "- `/path/to/your/files/**/another_dir/*` (this finds all the files contained in all sub-directories named `another_dir`)\n",
    "- `/path/to/your/files/**/another_dir/*png` (this finds all the pngs contained in all sub-directories named `another_dir`)\n",
    "\n",
    "`bucket`: string, name of bucket to write to. Also specifies the name of the experiment in the input database.\\\n",
    "`pth_to_s3_credentials`: string, path to AWS credentials file\\\n",
    "`data_root`: string, root path for data to upload\\\n",
    "`data_path`: string, path in data_root to be included in upload\\\n",
    "`multilevel`: True for multilevel directory structures, False if all data is stored in one directory\n",
    "`fam_trial_ids`: list of strings, stim_id for familiarization stimuli\\\n",
    "`batch_set_size`: int, # of stimuli to be included in each batch. should be a multiple of overall stimulus set size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example data used in this example is taken from [Physion](https://github.com/cogtoolslab/physics-benchmarking-neurips2021). Download [Physion_Dominoes](https://physics-benchmarking-neurips2021-dataset.s3.amazonaws.com/Physion_Dominoes.zip) (25 MB), extract it and copy the folder into the `stimuli/` subfolder of the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = (PROJECT + \"_\" + DATASET).replace(\"_\",\"-\").lower() # bucket name on AWS S3 where stimuli where be stored. `_` is not allowed in bucket names\n",
    "pth_to_s3_credentials = None # local path to your aws credentials in JSON format. Pass None to use shared credentials file\n",
    "data_root = \"/Users/felixbinder/Desktop/dominoes_all_movies 2/\"\n",
    "data_path = ['**/*_img.mp4'] # this finds all subdirectories in data_root and loads all files in each subdirectory to s3\n",
    "hdf5_path = ['**/*.hdf5'] # matching for the file containing metadata information\n",
    "multilevel=True # Dominoes/ contains 2 subdirectories, so the structure is multi-level\n",
    "stim_paths = ['*_map.png', '*_img.mp4', '*.hdf5'] # list of paths to stimuli to upload to s3—include a pattern to match only for relevant files\n",
    "batch_set_size = 150\n",
    "n_entries = 250 # how many different random orders do we want?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which stimuli IDs do we want to use for familiarization? Usually **2** stimuli are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fam_stim_ids = ['pilot_dominoes_1mid_J025R45_boxroom_0000',\n",
    "'pilot_dominoes_1mid_J025R45_boxroom_0006']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reproducibility, fix the random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload stimuli to S3\n",
    "We need to store the stimuli files in S3. This assumes that a bucket has already been created and the appropriate permissions have been set (the files need to be publicly available, as they are embedded by the web experiment.) \n",
    "\n",
    "Make sure that you have the appropriate credentials to upload to S3. \n",
    "\n",
    "Running this section will upload your stimuli files to the specified S3 bucket.\n",
    "\n",
    "Consider logging into the AWS console to make sure that the right files have been uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 159 paths to files\n",
      "['/Users/felixbinder/Desktop/dominoes_all_movies 2/pilot_dominoes_0mid_d3chairs_o1plants_tdwroom_0000_img.mp4', '/Users/felixbinder/Desktop/dominoes_all_movies 2/pilot_dominoes_0mid_d3chairs_o1plants_tdwroom_0001_img.mp4', '/Users/felixbinder/Desktop/dominoes_all_movies 2/pilot_dominoes_0mid_d3chairs_o1plants_tdwroom_0002_img.mp4', '/Users/felixbinder/Desktop/dominoes_all_movies 2/pilot_dominoes_0mid_d3chairs_o1plants_tdwroom_0004_img.mp4', '/Users/felixbinder/Desktop/dominoes_all_movies 2/pilot_dominoes_0mid_d3chairs_o1plants_tdwroom_0005_img.mp4'] \n",
      " ... \n",
      " ['/Users/felixbinder/Desktop/dominoes_all_movies 2/pilot_dominoes_default_boxroom_0014_img.mp4', '/Users/felixbinder/Desktop/dominoes_all_movies 2/pilot_dominoes_default_boxroom_0015_img.mp4', '/Users/felixbinder/Desktop/dominoes_all_movies 2/pilot_dominoes_default_boxroom_0022_img.mp4', '/Users/felixbinder/Desktop/dominoes_all_movies 2/pilot_dominoes_default_boxroom_0023_img.mp4', '/Users/felixbinder/Desktop/dominoes_all_movies 2/pilot_dominoes_default_boxroom_0025_img.mp4']\n"
     ]
    }
   ],
   "source": [
    "# which files would we upload?\n",
    "files = get_filepaths(data_root, data_path, multilevel)\n",
    "print(\"Got {} paths to files\".format(len(files)))\n",
    "try: print(files[0:5],\"\\n\",\"...\",\"\\n\",files[-5:])\n",
    "except: print(\"Not enough file paths to display\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 160 paths to files\n",
      "['/Users/felixbinder/Desktop/dominoes_all_movies 2/pilot_dominoes_0mid_d3chairs_o1plants_tdwroom_0000.hdf5', '/Users/felixbinder/Desktop/dominoes_all_movies 2/pilot_dominoes_0mid_d3chairs_o1plants_tdwroom_0001.hdf5'] \n",
      " ... \n",
      " ['/Users/felixbinder/Desktop/dominoes_all_movies 2/pilot_dominoes_default_boxroom_0023.hdf5', '/Users/felixbinder/Desktop/dominoes_all_movies 2/pilot_dominoes_default_boxroom_0025.hdf5']\n"
     ]
    }
   ],
   "source": [
    "hdf5_files = get_filepaths(data_root, hdf5_path, multilevel)\n",
    "print(\"Got {} paths to files\".format(len(hdf5_files)))\n",
    "try: print(hdf5_files[0:2],\"\\n\",\"...\",\"\\n\",hdf5_files[-2:])\n",
    "except: print(\"Not enough file paths to display\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload dataset to aws s3\n",
    "upload_stim_to_s3(bucket, \n",
    "                  pth_to_s3_credentials, \n",
    "                  data_root, \n",
    "                  stim_paths, \n",
    "                  multilevel,\n",
    "                  overwrite=False)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and upload session templates to the `input` database\n",
    "This section will create a number of session templates, and upload them to the `input` database. \n",
    "For purposes of documentation (or the use of app.js with `--local_store`) the file is also saved to disk.\n",
    "\n",
    "A session template is an ordered list of stimuli that will be shown to the participant. \n",
    "\n",
    "Make sure that you have appropriate credentials for the `input` database (see the documentation on the CAB config file). If you are not running this one the same machine as the database, you might need to create an ssh tunnel to the database server. (eg. run `ssh -fNL 27017:127.0.0.1:27017 USERNAME@cogtoolslab.org` in your terminal.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata...\n",
      "Parsing labels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [00:19<00:00,  8.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading `static`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [00:01<00:00, 99.62it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got data for 160 stimuli\n",
      "Loaded metadata for 160 stimuli\n",
      "Loaded S3 URLs for 160 stimuli\n",
      "Loaded familiarization stimuli for 2 stimuli\n",
      "Splitting stimulus set into batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/felixbinder/Cloud/Grad School/Fan Lab/BACH/cognitive-ai-benchmarking/stimuli/stimulus_setup.ipynb Cell 18\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/felixbinder/Cloud/Grad%20School/Fan%20Lab/BACH/cognitive-ai-benchmarking/stimuli/stimulus_setup.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# batch dataset and upload to mongodb\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/felixbinder/Cloud/Grad%20School/Fan%20Lab/BACH/cognitive-ai-benchmarking/stimuli/stimulus_setup.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m experiment_setup(project \u001b[39m=\u001b[39;49m PROJECT,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/felixbinder/Cloud/Grad%20School/Fan%20Lab/BACH/cognitive-ai-benchmarking/stimuli/stimulus_setup.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                  experiment \u001b[39m=\u001b[39;49m EXPERIMENT,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/felixbinder/Cloud/Grad%20School/Fan%20Lab/BACH/cognitive-ai-benchmarking/stimuli/stimulus_setup.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                  iteration \u001b[39m=\u001b[39;49m ITERATION,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/felixbinder/Cloud/Grad%20School/Fan%20Lab/BACH/cognitive-ai-benchmarking/stimuli/stimulus_setup.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                  bucket \u001b[39m=\u001b[39;49m bucket,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/felixbinder/Cloud/Grad%20School/Fan%20Lab/BACH/cognitive-ai-benchmarking/stimuli/stimulus_setup.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                  s3_stim_paths \u001b[39m=\u001b[39;49m stim_paths,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/felixbinder/Cloud/Grad%20School/Fan%20Lab/BACH/cognitive-ai-benchmarking/stimuli/stimulus_setup.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                  hdf5_paths \u001b[39m=\u001b[39;49m hdf5_files,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/felixbinder/Cloud/Grad%20School/Fan%20Lab/BACH/cognitive-ai-benchmarking/stimuli/stimulus_setup.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                  fam_trial_ids \u001b[39m=\u001b[39;49m fam_stim_ids,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/felixbinder/Cloud/Grad%20School/Fan%20Lab/BACH/cognitive-ai-benchmarking/stimuli/stimulus_setup.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                  batch_set_size \u001b[39m=\u001b[39;49m batch_set_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/felixbinder/Cloud/Grad%20School/Fan%20Lab/BACH/cognitive-ai-benchmarking/stimuli/stimulus_setup.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                  n_entries \u001b[39m=\u001b[39;49m n_entries,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/felixbinder/Cloud/Grad%20School/Fan%20Lab/BACH/cognitive-ai-benchmarking/stimuli/stimulus_setup.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                  overwrite \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/felixbinder/Cloud/Grad%20School/Fan%20Lab/BACH/cognitive-ai-benchmarking/stimuli/stimulus_setup.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                  exclude_fam_stem \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/Cloud/Grad School/Fan Lab/BACH/cognitive-ai-benchmarking/stimuli/experiment_config.py:208\u001b[0m, in \u001b[0;36mexperiment_setup\u001b[0;34m(project, experiment, iteration, bucket, s3_stim_paths, hdf5_paths, fam_trial_ids, batch_set_size, n_entries, overwrite, exclude_fam_stem)\u001b[0m\n\u001b[1;32m    205\u001b[0m M, M_fam, fam_trials \u001b[39m=\u001b[39m get_familiarization_stimuli(\n\u001b[1;32m    206\u001b[0m     M, fam_trial_ids, iteration)\n\u001b[1;32m    207\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoaded familiarization stimuli for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m stimuli\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mlen\u001b[39m(M_fam)))\n\u001b[0;32m--> 208\u001b[0m trial_data_sets \u001b[39m=\u001b[39m split_stim_set_to_batches(\n\u001b[1;32m    209\u001b[0m     batch_set_size, M, project, experiment, iteration, n_entries, fam_trial_ids, exclude_fam_stem)\n\u001b[1;32m    210\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSplit stimuli into \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m batches of \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m stimuli\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    211\u001b[0m     n_entries, batch_set_size))\n\u001b[1;32m    212\u001b[0m make_familiarization_json(M_fam, project, experiment, iteration)\n",
      "File \u001b[0;32m~/Cloud/Grad School/Fan Lab/BACH/cognitive-ai-benchmarking/stimuli/experiment_config.py:117\u001b[0m, in \u001b[0;36msplit_stim_set_to_batches\u001b[0;34m(batch_set_size, M, project, experiment, iteration, n_entries, fam_stim_ids, exclude_fam_stem)\u001b[0m\n\u001b[1;32m    115\u001b[0m old_len \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(M)\n\u001b[1;32m    116\u001b[0m \u001b[39mif\u001b[39;00m exclude_fam_stem:\n\u001b[0;32m--> 117\u001b[0m     fam_stim_ids \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(x\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39;49m\u001b[39m_\u001b[39;49m\u001b[39m\"\u001b[39;49m)[:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m fam_stim_ids))\n\u001b[1;32m    118\u001b[0m mask \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones(\u001b[39mlen\u001b[39m(M))\n\u001b[1;32m    119\u001b[0m \u001b[39mfor\u001b[39;00m fam_stim_id \u001b[39min\u001b[39;00m fam_stim_ids:\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "# batch dataset and upload to mongodb\n",
    "experiment_setup(project = PROJECT,\n",
    "                 experiment = EXPERIMENT,\n",
    "                 iteration = ITERATION,\n",
    "                 bucket = bucket,\n",
    "                 s3_stim_paths = stim_paths,\n",
    "                 hdf5_paths = hdf5_files,\n",
    "                 fam_trial_ids = fam_stim_ids,\n",
    "                 batch_set_size = batch_set_size,\n",
    "                 n_entries = n_entries,\n",
    "                 overwrite = True,\n",
    "                 exclude_fam_stem = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('curiophysion')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "7d490802aa9a2e5a123340609c5ae4c60c09c9e951ad0b74d9d85b02a78902d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
