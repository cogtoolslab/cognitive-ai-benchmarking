{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stimuli and session template setup notebook for OCP on Physion\n",
    "This notebook provides a minimal example on how to set up the stimuli and upload the sequence of trials to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from upload_to_s3 import upload_stim_to_s3, get_filepaths\n",
    "from experiment_config import experiment_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the names for the experiment and the iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"Physion_V1_5\" \n",
    "DATASET = \"Roll\"\n",
    "TASK = \"OCP_validation\"\n",
    "ITERATION = \"pilot_2\"\n",
    "EXPERIMENT = DATASET + \"_\" + TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The experiment should be reachable under the following URL:\n",
      "https://cogtoolslab.org:8882/OCP_readout/index.html?projName=Physion_V1_5&expName=Roll_OCP_readout&iterName=pilot_2\n",
      "The internal name is Physion_V1_5_Roll_OCP_readout_pilot_2\n"
     ]
    }
   ],
   "source": [
    "PORT = 8882 # which port did we use when launching `app.js` (ie. the `--gameport` flag)\n",
    "print(\"The experiment should be reachable under the following URL:\")\n",
    "print(\"https://cogtoolslab.org:{}/{}/index.html?projName={}&expName={}&iterName={}\".format(PORT,TASK,PROJECT,EXPERIMENT,ITERATION))\n",
    "print(\"The internal name is {}_{}_{}\".format(PROJECT,EXPERIMENT,ITERATION))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide metadata and locations of the stimuli files\n",
    "for a simple data directory with all to-be-uploaded files in one directory,  data_path is in the form /path/to/your/data\n",
    "    \n",
    "For a multi-level directory structure, you will need to use glob ** notation in data_path to index all the relevant files. something like:\n",
    "- `/path/to/your/files/**/*` (this finds all the files in your directory structure)\n",
    "- `/path/to/your/files/**/another_dir/*` (this finds all the files contained in all sub-directories named `another_dir`)\n",
    "- `/path/to/your/files/**/another_dir/*png` (this finds all the pngs contained in all sub-directories named `another_dir`)\n",
    "\n",
    "`bucket`: string, name of bucket to write to. Also specifies the name of the experiment in the input database.\\\n",
    "`pth_to_s3_credentials`: string, path to AWS credentials file\\\n",
    "`data_root`: string, root path for data to upload\\\n",
    "`data_path`: string, path in data_root to be included in upload\\\n",
    "`multilevel`: True for multilevel directory structures, False if all data is stored in one directory\n",
    "`fam_trial_ids`: list of strings, stim_id for familiarization stimuli\\\n",
    "`batch_set_size`: int, # of stimuli to be included in each batch. should be a multiple of overall stimulus set size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example data used in this example is taken from [Physion](https://github.com/cogtoolslab/physics-benchmarking-neurips2021). Download [Physion_Dominoes](https://physics-benchmarking-neurips2021-dataset.s3.amazonaws.com/Physion_Dominoes.zip) (25 MB), extract it and copy the folder into the `stimuli/` subfolder of the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = (PROJECT + \"_\" + DATASET + \"_\" + ITERATION).replace(\"_\",\"-\").lower() # bucket name on AWS S3 where stimuli where be stored. `_` is not allowed in bucket names\n",
    "pth_to_s3_credentials = None # local path to your aws credentials in JSON format. Pass None to use shared credentials file\n",
    "data_root = \"/Volumes/FJB Assets/Physion/v1_5/roll/physion-v1-5-roll-pilot-2\"\n",
    "data_path = ['**/*_img.mp4'] # this finds all subdirectories in data_root and loads all files in each subdirectory to s3\n",
    "hdf5_path = ['**/*.hdf5'] # matching for the file containing metadata information\n",
    "multilevel=True # Dominoes/ contains 2 subdirectories, so the structure is multi-level\n",
    "stim_paths = ['*_map.png', '*_img.mp4', '*.hdf5'] # list of paths to stimuli to upload to s3—include a pattern to match only for relevant files\n",
    "batch_set_size = 150\n",
    "n_entries = 250 # how many different random orders do we want?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DATASET.lower() in data_root:\n",
    "    raise ValueError(\"The dataset name ({}) does not match the data_root path ({})—are you sure you've updated the fields?\".format(DATASET, data_root))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which stimuli IDs do we want to use for familiarization? Usually **2** stimuli are used, one positive and one negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fam_stim_ids = [\n",
    "    'pilot_it2_rollingSliding_simple_collision_box_0005_img',\n",
    "    'pilot_it2_rollingSliding_simple_collision_box_0011_img'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reproducibility, fix the random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload stimuli to S3\n",
    "We need to store the stimuli files in S3. This assumes that a bucket has already been created and the appropriate permissions have been set (the files need to be publicly available, as they are embedded by the web experiment.) \n",
    "\n",
    "Make sure that you have the appropriate credentials to upload to S3. \n",
    "\n",
    "Running this section will upload your stimuli files to the specified S3 bucket.\n",
    "\n",
    "Consider logging into the AWS console to make sure that the right files have been uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 160 paths to files\n",
      "['/Volumes/FJB Assets/Physion/v1_5/roll/physion-v1-5-roll-pilot-2/pilot_it2_rollingSliding_simple_collision_box_0005_img.mp4', '/Volumes/FJB Assets/Physion/v1_5/roll/physion-v1-5-roll-pilot-2/pilot_it2_rollingSliding_simple_collision_box_0011_img.mp4', '/Volumes/FJB Assets/Physion/v1_5/roll/physion-v1-5-roll-pilot-2/pilot_it2_rollingSliding_simple_collision_box_0013_img.mp4', '/Volumes/FJB Assets/Physion/v1_5/roll/physion-v1-5-roll-pilot-2/pilot_it2_rollingSliding_simple_collision_box_0014_img.mp4', '/Volumes/FJB Assets/Physion/v1_5/roll/physion-v1-5-roll-pilot-2/pilot_it2_rollingSliding_simple_collision_box_0016_img.mp4'] \n",
      " ... \n",
      " ['/Volumes/FJB Assets/Physion/v1_5/roll/physion-v1-5-roll-pilot-2/pilot_it2_rollingSliding_simple_ramp_tdw_1_dis_1_occ_0031_img.mp4', '/Volumes/FJB Assets/Physion/v1_5/roll/physion-v1-5-roll-pilot-2/pilot_it2_rollingSliding_simple_ramp_tdw_1_dis_1_occ_0040_img.mp4', '/Volumes/FJB Assets/Physion/v1_5/roll/physion-v1-5-roll-pilot-2/pilot_it2_rollingSliding_simple_ramp_tdw_1_dis_1_occ_0041_img.mp4', '/Volumes/FJB Assets/Physion/v1_5/roll/physion-v1-5-roll-pilot-2/pilot_it2_rollingSliding_simple_ramp_tdw_1_dis_1_occ_0043_img.mp4', '/Volumes/FJB Assets/Physion/v1_5/roll/physion-v1-5-roll-pilot-2/pilot_it2_rollingSliding_simple_ramp_tdw_1_dis_1_occ_0045_img.mp4']\n"
     ]
    }
   ],
   "source": [
    "# which files would we upload?\n",
    "files = get_filepaths(data_root, data_path, multilevel)\n",
    "print(\"Got {} paths to files\".format(len(files)))\n",
    "try: print(files[0:5],\"\\n\",\"...\",\"\\n\",files[-5:])\n",
    "except: print(\"Not enough file paths to display\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 160 paths to files\n",
      "['/Volumes/FJB Assets/Physion/v1_5/roll/physion-v1-5-roll-pilot-2/pilot_it2_rollingSliding_simple_collision_box_0005.hdf5', '/Volumes/FJB Assets/Physion/v1_5/roll/physion-v1-5-roll-pilot-2/pilot_it2_rollingSliding_simple_collision_box_0011.hdf5'] \n",
      " ... \n",
      " ['/Volumes/FJB Assets/Physion/v1_5/roll/physion-v1-5-roll-pilot-2/pilot_it2_rollingSliding_simple_ramp_tdw_1_dis_1_occ_0043.hdf5', '/Volumes/FJB Assets/Physion/v1_5/roll/physion-v1-5-roll-pilot-2/pilot_it2_rollingSliding_simple_ramp_tdw_1_dis_1_occ_0045.hdf5']\n"
     ]
    }
   ],
   "source": [
    "hdf5_files = get_filepaths(data_root, hdf5_path, multilevel)\n",
    "print(\"Got {} paths to files\".format(len(hdf5_files)))\n",
    "try: print(hdf5_files[0:2],\"\\n\",\"...\",\"\\n\",hdf5_files[-2:])\n",
    "except: print(\"Not enough file paths to display\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(files) == len(hdf5_files), \"Number of files and hdf5 files do not match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do we have an mp4 and a png for each hdf5 file?\n",
    "import glob\n",
    "for f in hdf5_files:\n",
    "    for s in stim_paths:\n",
    "        if not glob.glob(f.replace(\".hdf5\",s)):\n",
    "            print(\"No match for {} in {}\".format(s,f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload dataset to aws s3\n",
    "upload_stim_to_s3(bucket, \n",
    "                  pth_to_s3_credentials, \n",
    "                  data_root, \n",
    "                  stim_paths, \n",
    "                  multilevel,\n",
    "                  overwrite=False)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and upload session templates to the `input` database\n",
    "This section will create a number of session templates, and upload them to the `input` database. \n",
    "For purposes of documentation (or the use of app.js with `--local_store`) the file is also saved to disk.\n",
    "\n",
    "A session template is an ordered list of stimuli that will be shown to the participant. \n",
    "\n",
    "Make sure that you have appropriate credentials for the `input` database (see the documentation on the CAB config file). If you are not running this one the same machine as the database, you might need to create an ssh tunnel to the database server. (eg. run `ssh -fNL 27017:127.0.0.1:27017 USERNAME@cogtoolslab.org` in your terminal.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata...\n",
      "Parsing labels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [00:30<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading `static`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [00:01<00:00, 95.00it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got data for 160 stimuli\n",
      "Loaded metadata for 160 stimuli\n",
      "Loaded S3 URLs for 160 stimuli\n",
      "Loaded familiarization stimuli for 2 stimuli\n",
      "Sampling 75 stimuli for each label for a total of 150 stimuli to ensure that each set contains the same stimuli. Label balancing applied.\n",
      "Excluded 0 familiarization stims from being chosen (beyond specific familiarization stims)\n",
      "Splitting stimulus set into batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:07<00:00, 35.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving—might take a second...\n",
      "Saved 250 batches of 150 stimuli to disk at Physion_V1_5_Roll_OCP_readout_trial_data_pilot_2.csv\n",
      "Split stimuli into 250 batches of 150 stimuli\n",
      "Running verifications\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<00:00, 8991.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifications passed\n",
      "Uploading to mongoDB with project Physion_V1_5_input, experiment Roll_OCP_readout, iteration pilot_2...\n",
      "Checking database connection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have the following collections: ['Collide_OCP', 'Contain_OCP', 'Dominoes_OCP', 'Drop_OCP', 'Link_OCP', 'Roll_OCP', 'Support_OCP']\n",
      "Deleted old entries of iteration pilot_2 from collection Roll_OCP_readout\n",
      "Uploading 250 batches of 150 stimuli to mongoDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [02:07<00:00,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done inserting records into database Physion_V1_5_input, collection Roll_OCP_readout. `.json` files have been saved to `stimuli/` folder.\n",
      "Uploaded stimuli to mongoDB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# batch dataset and upload to mongodb\n",
    "experiment_setup(project = PROJECT,\n",
    "                 experiment = EXPERIMENT,\n",
    "                 iteration = ITERATION,\n",
    "                 bucket = bucket,\n",
    "                 s3_stim_paths = stim_paths,\n",
    "                 hdf5_paths = hdf5_files,\n",
    "                 fam_trial_ids = fam_stim_ids,\n",
    "                 batch_set_size = batch_set_size,\n",
    "                 n_entries = n_entries,\n",
    "                 overwrite = True,\n",
    "                 exclude_fam_stem = False,\n",
    "                 ensure_same_stimuli = True,\n",
    "                 balance_stimuli = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('curiophysion')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "7d490802aa9a2e5a123340609c5ae4c60c09c9e951ad0b74d9d85b02a78902d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
